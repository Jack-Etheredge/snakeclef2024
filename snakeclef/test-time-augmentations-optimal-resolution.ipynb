{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377c7edd-c84e-4092-ab87-2a1188447f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/snakeclef-jsOUoRFY-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import ImageFile\n",
    "\n",
    "from closedset_model import build_model\n",
    "from competition_metrics import evaluate\n",
    "from datasets import get_valid_transform\n",
    "from paths import METADATA_DIR, VAL_DATA_DIR\n",
    "from utils import copy_config, get_device\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class PytorchWorker:\n",
    "    \"\"\"Run inference using PyTorch.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, number_of_categories: int = 1784, model_id=\"efficientnet_b0\", device=\"cpu\", transforms=None):\n",
    "\n",
    "        ########################################\n",
    "        # must be set before calling _load_model\n",
    "        self.number_of_categories = number_of_categories\n",
    "        self.model_id = model_id\n",
    "        self.device = device\n",
    "        ########################################\n",
    "\n",
    "        self.transforms = transforms\n",
    "        # most other attributes must be set before calling _load_model, so call last\n",
    "        self.model = self._load_model(model_path)\n",
    "\n",
    "    def _load_model(self, model_path):\n",
    "        print(\"Setting up Pytorch Model\")\n",
    "        # model = models.efficientnet_b0()\n",
    "        # model.classifier[1] = nn.Linear(in_features=1280, out_features=self.number_of_categories)\n",
    "        model = build_model(\n",
    "            model_id=self.model_id,\n",
    "            pretrained=False,\n",
    "            fine_tune=False,\n",
    "            num_classes=self.number_of_categories,\n",
    "            # this is all that matters. everything else will be overwritten by checkpoint state\n",
    "            dropout_rate=0.5,\n",
    "        ).to(self.device)\n",
    "        model_ckpt = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(model_ckpt['model_state_dict'])\n",
    "\n",
    "        return model.to(self.device).eval()\n",
    "\n",
    "    def predict_image(self, image: np.ndarray) -> list():\n",
    "        \"\"\"Run inference using ONNX runtime.\n",
    "\n",
    "        :param image: Input image as numpy array.\n",
    "        :return: A list with logits and confidences.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.transforms(image)\n",
    "        \n",
    "        if isinstance(img, tuple):\n",
    "            img = torch.cat([instance.unsqueeze(0) for instance in img])\n",
    "            img = torch.unique(img, dim=0)\n",
    "\n",
    "        if img.dim() < 4:\n",
    "            img = img.unsqueeze(0)\n",
    "        \n",
    "        img = img.to(self.device)\n",
    "        \n",
    "        logits = self.model(img)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_probas(test_metadata, model_id, model_path, images_root_path, device, transforms):\n",
    "    \"\"\"Make submission file\"\"\"\n",
    "\n",
    "    model = PytorchWorker(model_path, model_id=model_id, device=device, transforms=transforms)\n",
    "\n",
    "    probas_total = []\n",
    "    image_paths = test_metadata[\"image_path\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_path in tqdm(image_paths):\n",
    "            image_path = os.path.join(images_root_path, image_path)\n",
    "            test_image = Image.open(image_path).convert(\"RGB\")\n",
    "            logits = model.predict_image(test_image)\n",
    "            probas = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n",
    "            if probas.shape[0] > 1:\n",
    "                probas = np.mean(probas, axis=0)\n",
    "            probas = probas.squeeze()\n",
    "            probas_total.append(probas)\n",
    "    \n",
    "    return probas_total\n",
    "\n",
    "\n",
    "def evaluate_experiment(cfgs, trial_name=None, multi_instance=False, device=\"cpu\", multicrop=False, debug=False):\n",
    "\n",
    "    submission_file_path = \"test-time-augmengations-submission.csv\"\n",
    "    if trial_name is not None:\n",
    "        submission_file_path = trial_name + submission_file_path\n",
    "\n",
    "    metadata_file_path = METADATA_DIR / \"SnakeCLEF2023-ValMetadata.csv\"\n",
    "    test_metadata = pd.read_csv(metadata_file_path)\n",
    "    if debug:\n",
    "        test_metadata = test_metadata.head(20)\n",
    "    if not multi_instance:\n",
    "        test_metadata.drop_duplicates(\"observation_id\", keep=\"first\", inplace=True)\n",
    "    \n",
    "    probas_per_model = []\n",
    "    for cfg in cfgs:\n",
    "        experiment_id = cfg[\"experiment_id\"]\n",
    "        if debug: print(f\"getting probas for experiment {experiment_id}\")\n",
    "        model_id = cfg[\"model_id\"]\n",
    "        image_size = cfg[\"image_size\"]\n",
    "        transforms = get_valid_transform(image_size=image_size, pretrained=True, fivecrop=multicrop)\n",
    "        experiment_dir = Path(\"model_checkpoints\") / experiment_id\n",
    "        predictions_output_csv_path = str(experiment_dir / \"submission.csv\")\n",
    "        model_file = \"model.pth\"\n",
    "        model_path = str(experiment_dir / model_file)\n",
    "        probas = get_probas(\n",
    "            model_id=model_id,\n",
    "            test_metadata=test_metadata,\n",
    "            model_path=model_path,\n",
    "            images_root_path=VAL_DATA_DIR,\n",
    "            device=device,\n",
    "            transforms=transforms,\n",
    "        )\n",
    "        probas_per_model.append(probas)\n",
    "    probas_per_model = np.array(probas_per_model)\n",
    "    if debug: print(\"probas_per_model.shape\", probas_per_model.shape)\n",
    "    if len(cfgs) > 1:\n",
    "        averaged_probas = np.mean(probas_per_model, axis=0)\n",
    "    else:\n",
    "        averaged_probas = probas_per_model.squeeze()\n",
    "    if debug: print(\"averaged_probas.shape\", averaged_probas.shape)\n",
    "    # if debug: print(\"np.argmax(averaged_probas)\", np.argmax(averaged_probas))\n",
    "\n",
    "    if multi_instance:\n",
    "        preds = []\n",
    "        # pandas unique preserves order\n",
    "        for obs_id in test_metadata[\"observation_id\"].unique():\n",
    "            indices = list(test_metadata[\"observation_id\"].loc[lambda x: x==obs_id].index)\n",
    "            if len(indices) > 1:\n",
    "                if debug: print(\"indices\", indices)\n",
    "                observation_probas = averaged_probas[indices, :]\n",
    "                observation_average = np.mean(averaged_probas[indices], axis=0)\n",
    "                if debug: print(\"observation_average.shape\", observation_average.shape)\n",
    "                preds.extend([np.argmax(observation_average)] * len(indices))\n",
    "            else:\n",
    "                preds.append(np.argmax(averaged_probas[indices], axis=1)[0])\n",
    "    else:\n",
    "        preds = np.argmax(averaged_probas, axis=1)\n",
    "\n",
    "    if debug:\n",
    "        print(\"preds\", preds)\n",
    "        if isinstance(preds, list):\n",
    "            preds = np.array(preds)\n",
    "        print(\"preds.shape\", preds.shape)\n",
    "    \n",
    "    submission_df = test_metadata.copy()\n",
    "    submission_df[\"class_id\"] = preds\n",
    "    submission_df = submission_df[[\"observation_id\", \"class_id\"]]\n",
    "    submission_df.drop_duplicates(\"observation_id\", keep=\"first\", inplace=True)\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "    competition_metrics_scores = evaluate(\n",
    "        test_annotation_file=metadata_file_path,\n",
    "        user_submission_file=submission_file_path,\n",
    "        phase_codename=\"prediction-based\",\n",
    "    )[\"submission_result\"]\n",
    "\n",
    "    return competition_metrics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3c952-59ce-45e3-95e2-2920ec5b478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [07:58<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 48.97, 'Accuracy': 65.08, 'PSC': (32.99, 3.79, 9.16, 18.0), 'PSC_total': (2078, 239, 139, 273), 'Track1 Metric': 84.24, 'Track2 Metric': 3797}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [07:52<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 50.86, 'Accuracy': 66.88, 'PSC': (30.74, 3.89, 8.5, 18.39), 'PSC_total': (1936, 245, 129, 279), 'Track1 Metric': 84.82, 'Track2 Metric': 3629}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:30<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 52.22, 'Accuracy': 68.59, 'PSC': (29.58, 3.33, 7.32, 17.86), 'PSC_total': (1863, 210, 111, 271), 'Track1 Metric': 85.79, 'Track2 Metric': 3380}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:17<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 52.36, 'Accuracy': 68.83, 'PSC': (29.04, 3.68, 6.86, 17.86), 'PSC_total': (1829, 232, 104, 271), 'Track1 Metric': 85.99, 'Track2 Metric': 3355}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:27<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 53.42, 'Accuracy': 70.11, 'PSC': (28.0, 3.29, 6.92, 17.14), 'PSC_total': (1764, 207, 105, 260), 'Track1 Metric': 86.36, 'Track2 Metric': 3223}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:34<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 53.52, 'Accuracy': 70.51, 'PSC': (27.73, 3.18, 5.93, 17.67), 'PSC_total': (1747, 200, 90, 268), 'Track1 Metric': 86.77, 'Track2 Metric': 3133}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:43<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 53.51, 'Accuracy': 70.07, 'PSC': (27.81, 3.38, 6.0, 18.66), 'PSC_total': (1752, 213, 91, 283), 'Track1 Metric': 86.51, 'Track2 Metric': 3199}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:29<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 54.09, 'Accuracy': 70.65, 'PSC': (27.58, 2.98, 6.26, 18.06), 'PSC_total': (1737, 188, 95, 274), 'Track1 Metric': 86.65, 'Track2 Metric': 3136}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [08:20<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 53.14, 'Accuracy': 69.92, 'PSC': (27.94, 3.3, 6.39, 18.85), 'PSC_total': (1760, 208, 97, 286), 'Track1 Metric': 86.27, 'Track2 Metric': 3233}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7816/7816 [09:32<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation.....\n",
      "Evaluating for Prediction-based Phase\n",
      "Evaluated scores: {'F1 Score': 53.22, 'Accuracy': 70.43, 'PSC': (27.72, 3.06, 6.66, 17.86), 'PSC_total': (1746, 193, 101, 271), 'Track1 Metric': 86.4, 'Track2 Metric': 3179}\n",
      "Completed evaluation\n",
      "Using device: cuda:0\n",
      "Setting up Pytorch Model\n",
      "Not loading pre-trained weights\n",
      "Freezing hidden layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 4991/7816 [05:34<03:44, 12.59it/s]"
     ]
    }
   ],
   "source": [
    "performances = dict()\n",
    "start = 1\n",
    "stop = 2\n",
    "step = .1\n",
    "for mult in np.arange(start, stop + step, step):\n",
    "    res = int(mult*384)\n",
    "    cfgs = [\n",
    "    {\"model_id\": \"caformer_s18.sail_in22k_ft_in1k_384\",\n",
    "     \"experiment_id\": \"2024-05-05 22:41:41.115323\",\n",
    "    \"image_size\": res,},\n",
    "    ]\n",
    "    device = get_device()\n",
    "    scores = evaluate_experiment(cfgs=cfgs, trial_name=\"ensemble_multi-instance\",\n",
    "                                                multi_instance=False, multicrop=False, device=device, debug=False)\n",
    "    performances[res] = scores\n",
    "\n",
    "print(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577adc73-4ca6-430e-916e-874dbff36dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for res, perf in performances.items():\n",
    "    df = pd.DataFrame({\"res\": [res], \"Track 1\": [perf[\"Track1 Metric\"]]})\n",
    "    dfs.append(df)\n",
    "comparison_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cfcc0d0-e8db-4373-af45-a22e34992841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "      <th>Track 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>576</td>\n",
       "      <td>86.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>652</td>\n",
       "      <td>86.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>614</td>\n",
       "      <td>86.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>729</td>\n",
       "      <td>86.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>537</td>\n",
       "      <td>86.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>768</td>\n",
       "      <td>86.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>691</td>\n",
       "      <td>86.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>499</td>\n",
       "      <td>85.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>460</td>\n",
       "      <td>85.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>84.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384</td>\n",
       "      <td>84.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    res  Track 1\n",
       "5   576    86.77\n",
       "7   652    86.65\n",
       "6   614    86.51\n",
       "9   729    86.40\n",
       "4   537    86.36\n",
       "10  768    86.33\n",
       "8   691    86.27\n",
       "3   499    85.99\n",
       "2   460    85.79\n",
       "1   422    84.82\n",
       "0   384    84.24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.sort_values(\"Track 1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c154a2-f053-4f32-a791-112798865792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "      <th>Track 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>768</td>\n",
       "      <td>86.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>729</td>\n",
       "      <td>86.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>691</td>\n",
       "      <td>86.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>652</td>\n",
       "      <td>86.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>614</td>\n",
       "      <td>86.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>576</td>\n",
       "      <td>86.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>537</td>\n",
       "      <td>86.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>499</td>\n",
       "      <td>85.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>460</td>\n",
       "      <td>85.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422</td>\n",
       "      <td>84.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>384</td>\n",
       "      <td>84.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    res  Track 1\n",
       "10  768    86.33\n",
       "9   729    86.40\n",
       "8   691    86.27\n",
       "7   652    86.65\n",
       "6   614    86.51\n",
       "5   576    86.77\n",
       "4   537    86.36\n",
       "3   499    85.99\n",
       "2   460    85.79\n",
       "1   422    84.82\n",
       "0   384    84.24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df.sort_values(\"res\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2c083-ea5e-4c3b-8ee8-dfcf1e434f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakeclef",
   "language": "python",
   "name": "snakeclef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
